<html>
<head>
   <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
   <meta http-equiv="Pragma" content="no-cache">
   <meta http-equiv="Expires" content="0">
   <link href="https://fonts.googleapis.com/css?family=Crimson+Text" rel="stylesheet">
   <link href="style.css" rel="stylesheet" type="text/css">
   <title>links</title>
 </head>
 <body>
   
   <h2>Useful Link</a></h2>

   <a href="http://ckbjimmy.github.io/" target="_blank">Wei-Hung Weng</a><br>
   ckbjimmy [at] mit [dot] edu<br>
   <hr>

<!-- NLP -->
<a name="nlp">
<h3>NLP</h3>

<ul>
  <li> word embeddings
    <ul><li><a href="https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models" target="_blank">get pretrained word embeddings</a></li></ul>
    <ul><li><a href="https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/" target="_blank">gensim word2vec and doc2vec</a></li></ul>
    <ul><li><a href="http://www.lxway.com/812184552.htm" target="_blank">explain word2vec (cn)</a></li></ul>
    <ul><li><a href="https://github.com/nfmcclure/tensorflow_cookbook/blob/master/07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings/06_using_word2vec.ipynb" target="_blank">tensorflow word2vec embedding, embedding from other tf (good)</a></li></ul>
    <ul><li><a href="https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/" target="_blank">tensorflow GloVe embedding, embedding from other packages</a></li></ul>
  <li> github
    <ul><li><a href="https://github.com/Franck-Dernoncourt/NeuroNER" target="_blank">NeuroNER</a></li></ul>
    <ul><li><a href=""> </a></li></ul>
    <ul><li><a href=""> </a></li></ul>

</ul>
<hr>

<!-- ML -->
<a name="ml">
<h3>machine learning</h3>
<ul>
  <li> tensorflow
    <ul><li><a href="https://ireneli.eu/2016/05/15/tensorflow-05-understanding-basic-usage/#more-1132" target="_blank">very basic tf</a></li></ul>
    <ul><li><a href=""> </a></li></ul>
  <li> deep learning
    <ul><li><a href="http://www.kdnuggets.com/2017/07/when-not-use-deep-learning.html/2" target="_blank">why not DL</a></li></ul>
    <ul><li><a href="https://github.com/fchollet/keras/blob/53e541f7bf55de036f4f5641bd2947b96dd8c4c3/keras/metrics.py" target="_blank">keras metrics</a></li></ul>
    <ul><li><a href=""> </a></li></ul>
</ul>
<hr>  


<!-- Research -->
<a name="r">
<h3>research</h3>
<ul>
  <li> how to read papers
    <ul><li><a href="http://bangqu.com/2e2C66.html" target="_blank">Reddit 熱門話題：如何閱讀並理解論文中的數學內容？</a> - 當你閱讀一篇論文時，你不會只讀一次。你首先要閱讀標題，然後你要決定要不要讀摘要，讀完摘要你還要決定是否瀏覽結果，之後你再決定是否瀏覽整個文本，等等。人生苦短，要讀的文章實在太多了。要讀懂一篇有很多代數運算的論文，秘訣是不要在第一次閱讀時就去理解代數部分。假設它們是對的，然後繼續閱讀。另外，當你查看這些等式時，你要確保你能理解這些等式的實際意義。我相信你知道這些等式的數學含義，但你明白這些等式的物理意義嗎？你知道該怎樣用普通語言解釋給我聽嗎——這個等式說明了這個特定係統在做什麼？你可以說出這樣的話嗎——「當你最大化 ELBO，這個近似後驗將與似然項中的數據允許的先驗相似」？這就是 ELBO 表達式背後的「物理意義」。當你達到這種程度時，推理長長的代數操作就輕鬆多了。要怎樣達到這種程度？讀大量論文，做大量計算。別無他法。人們使用形式數學的原因是保證安全，防止出現什麼怪異的案例毀了他們的推理。至少對我來說，關鍵在於完全理解作者的意圖，然後再去研究公式。</li></ul>
    <ul><li><a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf" target="_blank">how to read a paper (very good)</a> - three pass, 1. read title, abstract, intro, section/subsection headings, conclusions, references, (5C: category, context, correctness, contributions, clarity) 2. figrues, tables, mark references,  ignore proof 3. reimplement, should jot down the future work</li></ul>
   <li> <a href="https://ckbjimmy.github.io/2017_cebu/cebu_workshop2.html" target="_blank">Example for hands-on exercise 2</a>
    <ul><li><a href="https://github.com/ckbjimmy/2017_cebu/raw/master/cebu_workshop2.Rmd" target="_blank">[script]</a> Right click download the script of hands-on exercise 2 and open in RStudio</li></ul>
</ul>

</body>
</html>