<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Wei-Hung Weng" />


<title>HST.953 Workshop 9: Exploratory Data Analysis</title>

<script src="workshop_9_eda_student_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="workshop_9_eda_student_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="workshop_9_eda_student_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="workshop_9_eda_student_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="workshop_9_eda_student_files/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="workshop_9_eda_student_files/highlight/default.css"
      type="text/css" />
<script src="workshop_9_eda_student_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="workshop_9_eda_student_files/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">HST.953 Workshop 9: Exploratory Data Analysis</h1>
<h4 class="author"><em>Wei-Hung Weng</em></h4>
<h4 class="date"><em>Oct 21, 2016</em></h4>

</div>


<div id="instructions" class="section level2">
<h2>Instructions:</h2>
<p>For those students taking the course for credit, the work done during this workshop is to be handed in. Please e-mail both your <code>Rmd</code> source file and <code>html</code> output to <a href="mailto:hst953hw@mit.edu">hst953hw@mit.edu</a> no later than Friday Oct. 28, 2016.</p>
<p><em>To complete the assignment, fill in necessary code in the places indicated with <code># Students: Insert your code here</code> and text based answers <code>### Student Answer</code> </em></p>
<p><strong>Before beginning</strong>, please test to see if the Rmd file will compile on your system by clicking the “Knit HTML button” in R studio above.</p>
<div class="figure">
<img src="knithtml.png" title="Where To Click" alt="" />

</div>
</div>
<div id="principles-of-exploratory-data-analysis-eda" class="section level1">
<h1>Principles of Exploratory Data Analysis (EDA)</h1>
<p>As stated in class, EDA’s goal is to better understand the data and the process by which it was generated.<br />
Within statistics, it is largely considered separate from inferential statistics (e.g., hypothesis testing, point and interval estimates, etc), where EDA has a very diverse and important set of goals:</p>
<ul>
<li>Provide an opportunity to do additional data cleaning.</li>
<li>Understand how the data is generated, and what the relationships between variables may be.</li>
<li>Suggest questions and hypotheses that can be subsequently answered and tested.</li>
<li>Identify what statistical methods may be most appropriate for the data to follow up with these questions and hypotheses.</li>
</ul>
<div id="cognitive-disfluency-make-it-work-for-you" class="section level2">
<h2>Cognitive Disfluency – make it work for you?</h2>
<p>There is often an urge due to productivity, laziness, or other factors to plow through with an analysis, using sophisticated analysis techniques to find the results you are looking for. With the proliferation of large datasets, this can be quite ineffective, as it largely separates the analyst from the data, resulting in misunderstanding or not understanding the data at all.</p>
<p>There is some evidence that <em>cognitive disfluency</em> (making it harder to learn) can lead to deeper learning. For analysts and data scientists this means slowing things down, often using basic (and sometimes tedious) methods to integrate the primary structure and relationships contained in the data, before pulling out the heavy machinery of modern data analysis.</p>
<p>When working with a new dataset I (Jesse) almost always start with what is discussed below.</p>
</div>
</div>
<div id="prerequisites" class="section level1">
<h1>Prerequisites</h1>
<p>Let’s start by reloading the data into a data frame called <code>dat</code>:</p>
<pre class="r"><code># If the following command doesn&#39;t work for you, please run file.choose(), find the aline dataset and replace what it outputs for &quot;aline-dataset.csv&quot; below:
dat &lt;- read.csv(&quot;aline-dataset.csv&quot;)</code></pre>
</div>
<div id="numerical-forms-of-eda" class="section level1">
<h1>Numerical Forms of EDA</h1>
<p>One form of EDA is to provide numerical summaries of the dataset. This can have many purposes:</p>
<ul>
<li>To verify the dataset you loaded is the one you think you did.</li>
<li>To quantify characteristics of the dataset which need to be reported numerically.</li>
</ul>
<div id="the-summary-function-in-r" class="section level2">
<h2>The <code>summary</code> function in <code>R</code></h2>
<p><code>R</code> has a very handy function, which performs differently for depending on the type of data structures you apply it to. This is the <code>summary</code> function, and it provides a very useful data summary of data frames.</p>
<pre class="r"><code>summary(dat)</code></pre>
<pre><code>##    subject_id       hadm_id         icustay_id          age        
##  Min.   :   12   Min.   :100016   Min.   :200019   Min.   : 16.02  
##  1st Qu.:14461   1st Qu.:124202   1st Qu.:225756   1st Qu.: 42.91  
##  Median :29381   Median :149787   Median :250863   Median : 57.83  
##  Mean   :39321   Mean   :149450   Mean   :250625   Mean   : 66.16  
##  3rd Qu.:64446   3rd Qu.:174744   3rd Qu.:276120   3rd Qu.: 74.39  
##  Max.   :99893   Max.   :199962   Max.   :299995   Max.   :300.02  
##                                                                    
##    gender_num                 icustay_intime day_icu_intime_num
##  Min.   :0.0000   2100-06-26 18:43:00:   1   Min.   :0.000     
##  1st Qu.:0.0000   2100-07-18 23:52:23:   1   1st Qu.:1.000     
##  Median :1.0000   2100-07-21 13:42:11:   1   Median :3.000     
##  Mean   :0.5791   2100-08-03 02:58:59:   1   Mean   :3.067     
##  3rd Qu.:1.0000   2100-08-08 12:35:15:   1   3rd Qu.:5.000     
##  Max.   :1.0000   2100-08-11 06:16:52:   1   Max.   :6.000     
##                   (Other)            :2745                     
##  hour_icu_intime            icustay_outtime  icu_los_day    
##  Min.   : 0.0    2100-06-28 16:23:00:   1   Min.   : 1.000  
##  1st Qu.: 6.0    2100-07-22 15:54:05:   1   1st Qu.: 1.682  
##  Median :14.0    2100-07-23 16:49:17:   1   Median : 2.430  
##  Mean   :12.9    2100-08-05 17:10:09:   1   Mean   : 3.488  
##  3rd Qu.:19.0    2100-08-09 23:33:44:   1   3rd Qu.: 4.077  
##  Max.   :23.0    2100-08-14 19:54:43:   1   Max.   :37.283  
##                  (Other)            :2745                   
##  hospital_los_day     hosp_exp_flg     icu_exp_flg         mort_day       
##  Min.   :  0.03819   Min.   :0.0000   Min.   :0.00000   Min.   :   0.001  
##  1st Qu.:  3.70833   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:   5.108  
##  Median :  6.34375   Median :0.0000   Median :0.00000   Median :  84.247  
##  Mean   :  8.17651   Mean   :0.1272   Mean   :0.08833   Mean   : 452.079  
##  3rd Qu.: 10.10000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.: 638.814  
##  Max.   :111.76528   Max.   :1.0000   Max.   :1.00000   Max.   :3731.972  
##                                                         NA&#39;s   :1752      
##    day_28_flg     mort_day_censored    censor_flg       aline_flg     
##  Min.   :0.0000   Min.   :   0.001   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.: 150.000   1st Qu.:0.0000   1st Qu.:0.0000  
##  Median :0.0000   Median : 150.000   Median :1.0000   Median :1.0000  
##  Mean   :0.1516   Mean   : 259.697   Mean   :0.6369   Mean   :0.5064  
##  3rd Qu.:0.0000   3rd Qu.: 150.000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :3731.972   Max.   :1.0000   Max.   :1.0000  
##                                                                       
##  aline_time_day    weight_first     height_first        bmi        
##  Min.   :0.0417   Min.   :  1.00   Min.   :0.640   Min.   : 10.22  
##  1st Qu.:0.0834   1st Qu.: 65.10   1st Qu.:1.630   1st Qu.: 23.64  
##  Median :0.1347   Median : 77.00   Median :1.702   Median : 26.92  
##  Mean   :0.3087   Mean   : 80.72   Mean   :1.700   Mean   : 29.08  
##  3rd Qu.:0.2533   3rd Qu.: 90.72   3rd Qu.:1.780   3rd Qu.: 31.58  
##  Max.   :9.8336   Max.   :710.00   Max.   :4.450   Max.   :190.31  
##  NA&#39;s   :1358     NA&#39;s   :1530     NA&#39;s   :1640    NA&#39;s   :2205    
##   service_unit    sofa_first       map_first         hr_first     
##  MED    :1031   Min.   : 0.000   Min.   :-17.00   Min.   : 38.00  
##  TRAUM  : 535   1st Qu.: 0.000   1st Qu.: 76.00   1st Qu.: 75.00  
##  NMED   : 357   Median : 2.000   Median : 86.00   Median : 87.00  
##  NSURG  : 278   Mean   : 2.652   Mean   : 87.41   Mean   : 87.91  
##  SURG   : 220   3rd Qu.: 4.000   3rd Qu.: 98.00   3rd Qu.:100.00  
##  CMED   : 188   Max.   :14.000   Max.   :310.00   Max.   :174.00  
##  (Other): 142                    NA&#39;s   :1141     NA&#39;s   :1041    
##    temp_first      spo2_first       cvp_first       bun_first    
##  Min.   : 0.00   Min.   : 14.00   Min.   : 0.00   Min.   :  1.0  
##  1st Qu.:36.11   1st Qu.: 98.00   1st Qu.: 7.00   1st Qu.: 12.0  
##  Median :36.67   Median :100.00   Median :10.00   Median : 16.0  
##  Mean   :36.66   Mean   : 98.41   Mean   :10.01   Mean   : 20.2  
##  3rd Qu.:37.33   3rd Qu.:100.00   3rd Qu.:12.50   3rd Qu.: 23.0  
##  Max.   :40.44   Max.   :100.00   Max.   :22.00   Max.   :139.0  
##  NA&#39;s   :1182    NA&#39;s   :1047     NA&#39;s   :2676    NA&#39;s   :348    
##  creatinine_first chloride_first    hgb_first     platelet_first  
##  Min.   : 0.100   Min.   : 56.0   Min.   : 3.60   Min.   :  12.0  
##  1st Qu.: 0.700   1st Qu.:100.0   1st Qu.:10.60   1st Qu.: 177.0  
##  Median : 0.900   Median :104.0   Median :12.30   Median : 232.0  
##  Mean   : 1.174   Mean   :103.9   Mean   :12.25   Mean   : 243.1  
##  3rd Qu.: 1.200   3rd Qu.:107.0   3rd Qu.:13.90   3rd Qu.: 296.0  
##  Max.   :18.800   Max.   :129.0   Max.   :20.20   Max.   :1313.0  
##  NA&#39;s   :349      NA&#39;s   :282     NA&#39;s   :249     NA&#39;s   :339     
##  potassium_first  sodium_first     tco2_first      wbc_first    
##  Min.   :1.800   Min.   : 74.0   Min.   : 3.00   Min.   : 0.20  
##  1st Qu.:3.600   1st Qu.:137.0   1st Qu.:22.00   1st Qu.: 8.00  
##  Median :4.000   Median :140.0   Median :25.00   Median :11.30  
##  Mean   :4.089   Mean   :139.3   Mean   :25.08   Mean   :12.27  
##  3rd Qu.:4.400   3rd Qu.:142.0   3rd Qu.:27.00   3rd Qu.:15.20  
##  Max.   :9.200   Max.   :172.0   Max.   :53.00   Max.   :94.00  
##  NA&#39;s   :254     NA&#39;s   :260     NA&#39;s   :210     NA&#39;s   :347    
##     chf_flg           afib_flg        renal_flg         liver_flg      
##  Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  
##  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  
##  Median :0.00000   Median :0.0000   Median :0.00000   Median :0.00000  
##  Mean   :0.08433   Mean   :0.1443   Mean   :0.06507   Mean   :0.05998  
##  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  
##  Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000  
##                                                                        
##     copd_flg          cad_flg         stroke_flg        mal_flg     
##  Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  
##  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  
##  Median :0.00000   Median :0.0000   Median :0.0000   Median :0.000  
##  Mean   :0.01818   Mean   :0.1334   Mean   :0.1454   Mean   :0.145  
##  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.000  
##  Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  
##                                                                     
##     resp_flg      endocarditis_flg    ards_flg       pneumonia_flg   
##  Min.   :0.0000   Min.   :0        Min.   :0.00000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0        1st Qu.:0.00000   1st Qu.:0.0000  
##  Median :0.0000   Median :0        Median :0.00000   Median :0.0000  
##  Mean   :0.3453   Mean   :0        Mean   :0.01672   Mean   :0.1745  
##  3rd Qu.:1.0000   3rd Qu.:0        3rd Qu.:0.00000   3rd Qu.:0.0000  
##  Max.   :1.0000   Max.   :0        Max.   :1.00000   Max.   :1.0000  
## </code></pre>
<p>As you can see, this function is very verbose, but produces some useful output. At this point, it’s also a good idea to verify the number of rows and columns are correct:</p>
<pre class="r"><code>nrow(dat)</code></pre>
<pre><code>## [1] 2751</code></pre>
<pre class="r"><code>ncol(dat)</code></pre>
<pre><code>## [1] 50</code></pre>
<p>This should be what you’re expecting (<code>2751</code> and <code>50</code>). If it’s not, this could indicate a loading error, or a problem with the data extraction.</p>
<p>As you will note, many of the <code>flg</code> variables listed in the summary output above, are constrained by 0 and 1. This is because they have a binary encoding (usually 1 if present, and 0 if not). Although not necessary in this particular instance, it it sometimes useful to encode these types of variables as factors. The function <code>convert.bin.fac</code> will do this, and we’ll use it to create a new data frame called <code>dat2</code>, and call the summary function on the new data frame. We first need to load/install two packages (<code>devtools</code> and <code>MIMICbook</code>).</p>
<pre class="r"><code>if(!(&quot;devtools&quot; %in% installed.packages()[,1])) {
  install.packages(&quot;devtools&quot;)
}
library(devtools)
if(!(&quot;MIMICbook&quot; %in% installed.packages()[,1])) {
  install_git(&quot;jraffa/MIMICbook&quot;)
}

library(MIMICbook)</code></pre>
<pre class="r"><code>dat2 &lt;- convert.bin.fac(dat)
summary(dat2)</code></pre>
<pre><code>##    subject_id       hadm_id         icustay_id          age        
##  Min.   :   12   Min.   :100016   Min.   :200019   Min.   : 16.02  
##  1st Qu.:14461   1st Qu.:124202   1st Qu.:225756   1st Qu.: 42.91  
##  Median :29381   Median :149787   Median :250863   Median : 57.83  
##  Mean   :39321   Mean   :149450   Mean   :250625   Mean   : 66.16  
##  3rd Qu.:64446   3rd Qu.:174744   3rd Qu.:276120   3rd Qu.: 74.39  
##  Max.   :99893   Max.   :199962   Max.   :299995   Max.   :300.02  
##                                                                    
##  gender_num             icustay_intime day_icu_intime_num hour_icu_intime
##  0:1158     2100-06-26 18:43:00:   1   Min.   :0.000      Min.   : 0.0   
##  1:1593     2100-07-18 23:52:23:   1   1st Qu.:1.000      1st Qu.: 6.0   
##             2100-07-21 13:42:11:   1   Median :3.000      Median :14.0   
##             2100-08-03 02:58:59:   1   Mean   :3.067      Mean   :12.9   
##             2100-08-08 12:35:15:   1   3rd Qu.:5.000      3rd Qu.:19.0   
##             2100-08-11 06:16:52:   1   Max.   :6.000      Max.   :23.0   
##             (Other)            :2745                                     
##             icustay_outtime  icu_los_day     hospital_los_day   
##  2100-06-28 16:23:00:   1   Min.   : 1.000   Min.   :  0.03819  
##  2100-07-22 15:54:05:   1   1st Qu.: 1.682   1st Qu.:  3.70833  
##  2100-07-23 16:49:17:   1   Median : 2.430   Median :  6.34375  
##  2100-08-05 17:10:09:   1   Mean   : 3.488   Mean   :  8.17651  
##  2100-08-09 23:33:44:   1   3rd Qu.: 4.077   3rd Qu.: 10.10000  
##  2100-08-14 19:54:43:   1   Max.   :37.283   Max.   :111.76528  
##  (Other)            :2745                                       
##  hosp_exp_flg icu_exp_flg    mort_day        day_28_flg mort_day_censored 
##  0:2401       0:2508      Min.   :   0.001   0:2334     Min.   :   0.001  
##  1: 350       1: 243      1st Qu.:   5.108   1: 417     1st Qu.: 150.000  
##                           Median :  84.247              Median : 150.000  
##                           Mean   : 452.079              Mean   : 259.697  
##                           3rd Qu.: 638.814              3rd Qu.: 150.000  
##                           Max.   :3731.972              Max.   :3731.972  
##                           NA&#39;s   :1752                                    
##  censor_flg aline_flg aline_time_day    weight_first     height_first  
##  0: 999     0:1358    Min.   :0.0417   Min.   :  1.00   Min.   :0.640  
##  1:1752     1:1393    1st Qu.:0.0834   1st Qu.: 65.10   1st Qu.:1.630  
##                       Median :0.1347   Median : 77.00   Median :1.702  
##                       Mean   :0.3087   Mean   : 80.72   Mean   :1.700  
##                       3rd Qu.:0.2533   3rd Qu.: 90.72   3rd Qu.:1.780  
##                       Max.   :9.8336   Max.   :710.00   Max.   :4.450  
##                       NA&#39;s   :1358     NA&#39;s   :1530     NA&#39;s   :1640   
##       bmi          service_unit    sofa_first       map_first     
##  Min.   : 10.22   MED    :1031   Min.   : 0.000   Min.   :-17.00  
##  1st Qu.: 23.64   TRAUM  : 535   1st Qu.: 0.000   1st Qu.: 76.00  
##  Median : 26.92   NMED   : 357   Median : 2.000   Median : 86.00  
##  Mean   : 29.08   NSURG  : 278   Mean   : 2.652   Mean   : 87.41  
##  3rd Qu.: 31.58   SURG   : 220   3rd Qu.: 4.000   3rd Qu.: 98.00  
##  Max.   :190.31   CMED   : 188   Max.   :14.000   Max.   :310.00  
##  NA&#39;s   :2205     (Other): 142                    NA&#39;s   :1141    
##     hr_first        temp_first      spo2_first       cvp_first    
##  Min.   : 38.00   Min.   : 0.00   Min.   : 14.00   Min.   : 0.00  
##  1st Qu.: 75.00   1st Qu.:36.11   1st Qu.: 98.00   1st Qu.: 7.00  
##  Median : 87.00   Median :36.67   Median :100.00   Median :10.00  
##  Mean   : 87.91   Mean   :36.66   Mean   : 98.41   Mean   :10.01  
##  3rd Qu.:100.00   3rd Qu.:37.33   3rd Qu.:100.00   3rd Qu.:12.50  
##  Max.   :174.00   Max.   :40.44   Max.   :100.00   Max.   :22.00  
##  NA&#39;s   :1041     NA&#39;s   :1182    NA&#39;s   :1047     NA&#39;s   :2676   
##    bun_first     creatinine_first chloride_first    hgb_first    
##  Min.   :  1.0   Min.   : 0.100   Min.   : 56.0   Min.   : 3.60  
##  1st Qu.: 12.0   1st Qu.: 0.700   1st Qu.:100.0   1st Qu.:10.60  
##  Median : 16.0   Median : 0.900   Median :104.0   Median :12.30  
##  Mean   : 20.2   Mean   : 1.174   Mean   :103.9   Mean   :12.25  
##  3rd Qu.: 23.0   3rd Qu.: 1.200   3rd Qu.:107.0   3rd Qu.:13.90  
##  Max.   :139.0   Max.   :18.800   Max.   :129.0   Max.   :20.20  
##  NA&#39;s   :348     NA&#39;s   :349      NA&#39;s   :282     NA&#39;s   :249    
##  platelet_first   potassium_first  sodium_first     tco2_first   
##  Min.   :  12.0   Min.   :1.800   Min.   : 74.0   Min.   : 3.00  
##  1st Qu.: 177.0   1st Qu.:3.600   1st Qu.:137.0   1st Qu.:22.00  
##  Median : 232.0   Median :4.000   Median :140.0   Median :25.00  
##  Mean   : 243.1   Mean   :4.089   Mean   :139.3   Mean   :25.08  
##  3rd Qu.: 296.0   3rd Qu.:4.400   3rd Qu.:142.0   3rd Qu.:27.00  
##  Max.   :1313.0   Max.   :9.200   Max.   :172.0   Max.   :53.00  
##  NA&#39;s   :339      NA&#39;s   :254     NA&#39;s   :260     NA&#39;s   :210    
##    wbc_first     chf_flg  afib_flg renal_flg liver_flg copd_flg cad_flg 
##  Min.   : 0.20   0:2519   0:2354   0:2572    0:2586    0:2701   0:2384  
##  1st Qu.: 8.00   1: 232   1: 397   1: 179    1: 165    1:  50   1: 367  
##  Median :11.30                                                          
##  Mean   :12.27                                                          
##  3rd Qu.:15.20                                                          
##  Max.   :94.00                                                          
##  NA&#39;s   :347                                                            
##  stroke_flg mal_flg  resp_flg endocarditis_flg ards_flg pneumonia_flg
##  0:2351     0:2352   0:1801   0:2751           0:2705   0:2271       
##  1: 400     1: 399   1: 950                    1:  46   1: 480       
##                                                                      
##                                                                      
##                                                                      
##                                                                      
## </code></pre>
<p>As you can now see, instead of means (which under the old encoding equate to proportions of patients where the variable == 1), now we have counts of patients with each <em>level</em> of the variable. This is because <code>R</code>’s summary function treats factors and numerics differently.</p>
<p>Often, you will want to report these summaries separately for different groups. For instance, is the mean or median age the same for those who received aline, and those who didn’t? A multi-purpose function called <code>tapply</code> can help us with this.</p>
<pre class="r"><code>tapply(dat2$age,dat2$aline_flg,summary)</code></pre>
<pre><code>## $`0`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.78   42.13   57.31   70.74   75.24  300.00 
## 
## $`1`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.02   43.69   58.26   61.69   73.30  300.00</code></pre>
<p>This function stratifies the first argument (<code>age</code>) by the second argument (<code>aline_flg</code>) and run the third argument (<code>summary</code>) on it. So, in our case, run the <code>summary</code> function on <code>age</code> for those who received aline (<code>aline_flg</code> = 1) and those who didn’t (<code>aline_flg</code> = 0).</p>
<div id="student-question-1" class="section level3">
<h3>Student Question 1:</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Using the <code>dat2</code> data frame, run the summary function for <code>sofa_first</code>, and <code>service_unit</code> separately for those with an aline, and those without.<br />
</li>
<li>Then run the summary function for <code>age</code> <code>sofa_first</code>, and <code>service_unit</code> separately for those who died within 28 days, and those who survived.</li>
<li>In three (3) sentences or less, discuss these findings, and note anything interesting.</li>
</ol>
</blockquote>
</div>
<div id="student-answer-1" class="section level3">
<h3>Student Answer 1:</h3>
<pre class="r"><code>tapply(dat2$sofa_first, dat2$aline_flg, summary)</code></pre>
<pre><code>## $`0`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   2.000   2.615   4.000  14.000 
## 
## $`1`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   2.000   2.688   4.000  14.000</code></pre>
<pre class="r"><code>tapply(dat2$service_unit, dat2$aline_flg, summary)</code></pre>
<pre><code>## $`0`
##  CMED  DENT   ENT    GU   GYN   MED  NMED NSURG   OBS  OMED ORTHO PSURG 
##    64     1    24     1     6   739   185    93     1     8    13     7 
##  SURG TRAUM 
##    43   173 
## 
## $`1`
##  CMED  DENT   ENT    GU   GYN   MED  NMED NSURG   OBS  OMED ORTHO PSURG 
##   124     1    10    13     3   292   172   185     1     5    39     9 
##  SURG TRAUM 
##   177   362</code></pre>
<pre class="r"><code>tapply(dat2$age, dat2$day_28_flg, summary)</code></pre>
<pre><code>## $`0`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.02   40.13   54.36   60.27   70.54  300.00 
## 
## $`1`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   22.06   64.52   77.00   99.14   84.45  300.00</code></pre>
<pre class="r"><code>tapply(dat2$sofa_first, dat2$day_28_flg, summary)</code></pre>
<pre><code>## $`0`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   2.000   2.582   4.000  14.000 
## 
## $`1`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   3.000   3.043   5.000  14.000</code></pre>
<pre class="r"><code>tapply(dat2$service_unit, dat2$day_28_flg, summary)</code></pre>
<pre><code>## $`0`
##  CMED  DENT   ENT    GU   GYN   MED  NMED NSURG   OBS  OMED ORTHO PSURG 
##   165     2    34    14     9   886   233   204     2     4    51    16 
##  SURG TRAUM 
##   212   502 
## 
## $`1`
##  CMED  DENT   ENT    GU   GYN   MED  NMED NSURG   OBS  OMED ORTHO PSURG 
##    23     0     0     0     0   145   124    74     0     9     1     0 
##  SURG TRAUM 
##     8    33</code></pre>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>SOFA score is slightly higher in aline usage group. CMED, GU, NSURG, ORTHO, SURG and TRAUM units have higher aline usage. Higher age, SOFA score, and staying in NMED or OMED unit are related to higher 28-day mortality.</li>
</ol></li>
</ul>
</div>
</div>
<div id="producing-a-table-one-and-other-tables" class="section level2">
<h2>Producing a Table One and Other Tables</h2>
<p>The output from summary is very useful, but is generally not acceptable formal research reports, let alone a published paper. There are several ways to produce a publication which has a better layout. One way is described in the text book (Chapter 15), another, which we will cover here is through an <code>R</code> package called <code>tableone</code>.</p>
<p>As some of you may know, “Table 1” often refers to the table presented in most medical manuscripts which contains information used to describe the cohort. This often includes information about average patient age, gender distribution, and other important demographic, clinical and socioeconomic characteristics. We will cover briefly how to use the <code>CreateTableOne</code> function in this package to generate a table which is closer to being publication worthy.</p>
<p>The following code will install the <code>tableone</code> package and load it.</p>
<pre class="r"><code>if(!(&quot;tableone&quot; %in% installed.packages()[,1])) {
  install.packages(&quot;tableone&quot;)
}
library(tableone)</code></pre>
<p>Here is an example functional call to <code>CreateTableOne</code>, which computes either the mean and standard deviation for numeric variables, or count and percentage for factors. You specify which variables you want to include in the table</p>
<pre class="r"><code>CreateTableOne(vars=c(&quot;age&quot;,&quot;service_unit&quot;,&quot;aline_flg&quot;,&quot;day_28_flg&quot;),data=dat2)</code></pre>
<pre><code>##                     
##                      Overall      
##   n                   2751        
##   age (mean (sd))    66.16 (52.62)
##   service_unit (%)                
##      CMED              188 ( 6.8) 
##      DENT                2 ( 0.1) 
##      ENT                34 ( 1.2) 
##      GU                 14 ( 0.5) 
##      GYN                 9 ( 0.3) 
##      MED              1031 (37.5) 
##      NMED              357 (13.0) 
##      NSURG             278 (10.1) 
##      OBS                 2 ( 0.1) 
##      OMED               13 ( 0.5) 
##      ORTHO              52 ( 1.9) 
##      PSURG              16 ( 0.6) 
##      SURG              220 ( 8.0) 
##      TRAUM             535 (19.4) 
##   aline_flg = 1 (%)   1393 (50.6) 
##   day_28_flg = 1 (%)   417 (15.2)</code></pre>
<p>We may want to breakdown these summaries further, like we did above with <code>tapply</code>, but we can do it with one function with the <code>CreateTableOne</code> function by passing the <code>strata</code> parameter. <code>strata</code> specifies which variable to stratify (breakdown) the others by. For example, here is the same table in the previous chunk, broken down by whether a patient received an aline or not.</p>
<pre class="r"><code>CreateTableOne(vars=c(&quot;age&quot;,&quot;service_unit&quot;,&quot;aline_flg&quot;,&quot;day_28_flg&quot;),strata=&quot;aline_flg&quot;,data=dat2,test=FALSE)</code></pre>
<pre><code>##                     Stratified by aline_flg
##                      0             1             
##   n                   1358          1393         
##   age (mean (sd))    70.74 (62.19) 61.69 (40.74) 
##   service_unit (%)                               
##      CMED               64 ( 4.7)    124 (  8.9) 
##      DENT                1 ( 0.1)      1 (  0.1) 
##      ENT                24 ( 1.8)     10 (  0.7) 
##      GU                  1 ( 0.1)     13 (  0.9) 
##      GYN                 6 ( 0.4)      3 (  0.2) 
##      MED               739 (54.4)    292 ( 21.0) 
##      NMED              185 (13.6)    172 ( 12.3) 
##      NSURG              93 ( 6.8)    185 ( 13.3) 
##      OBS                 1 ( 0.1)      1 (  0.1) 
##      OMED                8 ( 0.6)      5 (  0.4) 
##      ORTHO              13 ( 1.0)     39 (  2.8) 
##      PSURG               7 ( 0.5)      9 (  0.6) 
##      SURG               43 ( 3.2)    177 ( 12.7) 
##      TRAUM             173 (12.7)    362 ( 26.0) 
##   aline_flg = 1 (%)      0 ( 0.0)   1393 (100.0) 
##   day_28_flg = 1 (%)   206 (15.2)    211 ( 15.1)</code></pre>
<div id="student-question-2" class="section level3">
<h3>Student Question 2:</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Compute a Table to summarise those variable considered before (<code>age</code>, <code>service_unit</code>, <code>aline_flg</code> and <code>day_28_flg</code>) in addition to <code>gender_num</code> and <code>chf_flg</code>, but now stratify by survival at 28 days (<code>day_28_flg</code>).</li>
<li>In 1-2 sentences, do you notice anything interesting or peculiar.</li>
<li>Repeat part a), but now use the <code>dat</code> data frame instead of <code>dat2</code>. Briefly discuss the differences and which you prefer.</li>
</ol>
</blockquote>
</div>
<div id="student-answer-2" class="section level3">
<h3>Student Answer 2:</h3>
<pre class="r"><code>CreateTableOne(vars=c(&quot;age&quot;, &quot;service_unit&quot;, &quot;aline_flg&quot;, &quot;day_28_flg&quot;, &quot;gender_num&quot;, &quot;chf_flg&quot;),
               strata=&quot;day_28_flg&quot;,
               data=dat2, test=FALSE)</code></pre>
<pre><code>##                     Stratified by day_28_flg
##                      0             1             
##   n                   2334           417         
##   age (mean (sd))    60.27 (44.44) 99.14 (77.11) 
##   service_unit (%)                               
##      CMED              165 ( 7.1)     23 (  5.5) 
##      DENT                2 ( 0.1)      0 (  0.0) 
##      ENT                34 ( 1.5)      0 (  0.0) 
##      GU                 14 ( 0.6)      0 (  0.0) 
##      GYN                 9 ( 0.4)      0 (  0.0) 
##      MED               886 (38.0)    145 ( 34.8) 
##      NMED              233 (10.0)    124 ( 29.7) 
##      NSURG             204 ( 8.7)     74 ( 17.7) 
##      OBS                 2 ( 0.1)      0 (  0.0) 
##      OMED                4 ( 0.2)      9 (  2.2) 
##      ORTHO              51 ( 2.2)      1 (  0.2) 
##      PSURG              16 ( 0.7)      0 (  0.0) 
##      SURG              212 ( 9.1)      8 (  1.9) 
##      TRAUM             502 (21.5)     33 (  7.9) 
##   aline_flg = 1 (%)   1182 (50.6)    211 ( 50.6) 
##   day_28_flg = 1 (%)     0 ( 0.0)    417 (100.0) 
##   gender_num = 1 (%)  1389 (59.5)    204 ( 48.9) 
##   chf_flg = 1 (%)      207 ( 8.9)     25 (  6.0)</code></pre>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>The patients admitted to NMED, NSURG or OMED units, or older patients, have higher 28-day mortality. <code>aline_flg</code> should be removed from variable list since it is the outcome variable.</li>
</ol></li>
</ul>
<pre class="r"><code>CreateTableOne(vars=c(&quot;age&quot;, &quot;service_unit&quot;, &quot;aline_flg&quot;, &quot;day_28_flg&quot;, &quot;gender_num&quot;, &quot;chf_flg&quot;),
               strata=&quot;day_28_flg&quot;,
               data=dat, test=FALSE)</code></pre>
<pre><code>##                         Stratified by day_28_flg
##                          0             1            
##   n                       2334           417        
##   age (mean (sd))        60.27 (44.44) 99.14 (77.11)
##   service_unit (%)                                  
##      CMED                  165 ( 7.1)     23 ( 5.5) 
##      DENT                    2 ( 0.1)      0 ( 0.0) 
##      ENT                    34 ( 1.5)      0 ( 0.0) 
##      GU                     14 ( 0.6)      0 ( 0.0) 
##      GYN                     9 ( 0.4)      0 ( 0.0) 
##      MED                   886 (38.0)    145 (34.8) 
##      NMED                  233 (10.0)    124 (29.7) 
##      NSURG                 204 ( 8.7)     74 (17.7) 
##      OBS                     2 ( 0.1)      0 ( 0.0) 
##      OMED                    4 ( 0.2)      9 ( 2.2) 
##      ORTHO                  51 ( 2.2)      1 ( 0.2) 
##      PSURG                  16 ( 0.7)      0 ( 0.0) 
##      SURG                  212 ( 9.1)      8 ( 1.9) 
##      TRAUM                 502 (21.5)     33 ( 7.9) 
##   aline_flg (mean (sd))   0.51 (0.50)   0.51 (0.50) 
##   day_28_flg (mean (sd))  0.00 (0.00)   1.00 (0.00) 
##   gender_num (mean (sd))  0.60 (0.49)   0.49 (0.50) 
##   chf_flg (mean (sd))     0.09 (0.28)   0.06 (0.24)</code></pre>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>The binary variable can’t present well if we didn’t convert the data format. The value of these variables are actually the proportion, and the proportions show here are not meaningful. Using <code>dat</code> table can’t get the absolute count</li>
</ol></li>
</ul>
<div id="optional" class="section level4">
<h4>Optional:</h4>
<blockquote>
<p>As an aside, the following code may help for your projects, as it improves the presentation of the tables above. You will still need to update the column and row names manually, but this should paste nicely into Word or LateX!</p>
</blockquote>
<pre class="r"><code> if(!(&quot;dplyr&quot; %in% installed.packages()[,1])) {
 install.packages(&quot;dplyr&quot;)
 }
library(dplyr)
CreateTableOne(vars=c(&quot;age&quot;,&quot;service_unit&quot;,&quot;aline_flg&quot;,&quot;day_28_flg&quot;),strata=&quot;aline_flg&quot;,data=dat2,test=FALSE) %&gt;% print(
  printToggle      = FALSE,
  showAllLevels    = TRUE,
  cramVars         = &quot;kon&quot;
) %&gt;% 
{data.frame(
  variable_name             = gsub(&quot; &quot;, &quot;&amp;nbsp;&quot;, rownames(.), fixed = TRUE), ., 
  row.names        = NULL, 
  check.names      = FALSE, 
  stringsAsFactors = FALSE)} %&gt;% 
knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable_name</th>
<th align="left">level</th>
<th align="left">0</th>
<th align="left">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">n</td>
<td align="left"></td>
<td align="left">1358</td>
<td align="left">1393</td>
</tr>
<tr class="even">
<td align="left">age (mean (sd))</td>
<td align="left"></td>
<td align="left">70.74 (62.19)</td>
<td align="left">61.69 (40.74)</td>
</tr>
<tr class="odd">
<td align="left">service_unit (%)</td>
<td align="left">CMED</td>
<td align="left">64 ( 4.7)</td>
<td align="left">124 ( 8.9)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">DENT</td>
<td align="left">1 ( 0.1)</td>
<td align="left">1 ( 0.1)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">ENT</td>
<td align="left">24 ( 1.8)</td>
<td align="left">10 ( 0.7)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">GU</td>
<td align="left">1 ( 0.1)</td>
<td align="left">13 ( 0.9)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">GYN</td>
<td align="left">6 ( 0.4)</td>
<td align="left">3 ( 0.2)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">MED</td>
<td align="left">739 ( 54.4)</td>
<td align="left">292 ( 21.0)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">NMED</td>
<td align="left">185 ( 13.6)</td>
<td align="left">172 ( 12.3)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">NSURG</td>
<td align="left">93 ( 6.8)</td>
<td align="left">185 ( 13.3)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">OBS</td>
<td align="left">1 ( 0.1)</td>
<td align="left">1 ( 0.1)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">OMED</td>
<td align="left">8 ( 0.6)</td>
<td align="left">5 ( 0.4)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">ORTHO</td>
<td align="left">13 ( 1.0)</td>
<td align="left">39 ( 2.8)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">PSURG</td>
<td align="left">7 ( 0.5)</td>
<td align="left">9 ( 0.6)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">SURG</td>
<td align="left">43 ( 3.2)</td>
<td align="left">177 ( 12.7)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">TRAUM</td>
<td align="left">173 ( 12.7)</td>
<td align="left">362 ( 26.0)</td>
</tr>
<tr class="odd">
<td align="left">aline_flg (%)</td>
<td align="left">0</td>
<td align="left">1358 (100.0)</td>
<td align="left">0 ( 0.0)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">1</td>
<td align="left">0 ( 0.0)</td>
<td align="left">1393 (100.0)</td>
</tr>
<tr class="odd">
<td align="left">day_28_flg (%)</td>
<td align="left">0</td>
<td align="left">1152 ( 84.8)</td>
<td align="left">1182 ( 84.9)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">1</td>
<td align="left">206 ( 15.2)</td>
<td align="left">211 ( 15.1)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="other-bivariate-numerical-summaries" class="section level2">
<h2>Other Bivariate Numerical Summaries</h2>
<p>Sometimes you may wish to display the relationships between two or more variables directly. For categorical variables this can be tricky. One common way to explore relationships between categorical variables is by producing the cross tabulated tables (“crosstabs” for short). This is mainly done via the <code>table</code> function, which can take several categorical variables, and produce the number of patients which meet criteria for those variables. For instance, looking at how aline was used in men and women:</p>
<pre class="r"><code>table(dat2$gender_num,dat2$aline_flg,dnn=c(&quot;Gender&quot;,&quot;Aline&quot;))</code></pre>
<pre><code>##       Aline
## Gender   0   1
##      0 596 562
##      1 762 831</code></pre>
<p>we can see that aline was used 831 times in men (<code>gender_num=1</code>) and 762 times in women (<code>gender_num=0</code>). The raw numbers are often difficult to compare, so often the proportions are more useful. Applying <code>prop.table</code> to our existing table, and adding the argument 1 (for by row, use 2 for columns), we get the proportion of men and women who had aline (48% vs. 52%).</p>
<pre class="r"><code>prop.table(table(dat2$gender_num,dat2$aline_flg,dnn=c(&quot;Gender&quot;,&quot;Aline&quot;)),1)</code></pre>
<pre><code>##       Aline
## Gender         0         1
##      0 0.5146805 0.4853195
##      1 0.4783427 0.5216573</code></pre>
<p>A different summary for the relationships exists when dealing bivariate numeric data. Sometimes it’s desirable to present the strength of the relationship between two variables, and correlation coefficient is the way to go about this. There is a <code>cor</code> function in <code>R</code>, but when dealing with only two variables, it’s easiest to use the <code>cor.test</code> function. Under the defaults, it computes the Pearson product-moment correlation and computes a hypothesis test (covered briefly earlier today) to assess if there’s evidence that the correlation is not zero. Alternative forms of correlation are computed below, including Spearman’s rho and Kendall’s tau. These latter methods are useful when dealing with data which is not necessarily numeric but ordered (e.g., likert based rankings on a 1-5 scale) or has outliers. Spearman’s rho and Kendall’s tau are rank based methods, and also have a certain degree of robustness to outliers in the data. None of these methods are robust to non-linear relationships, and it’s very easy to miss a strong relationship between two variables if you rely on these methods in isolation.</p>
<pre class="r"><code>cor.test(dat2$bun_first,dat2$creatinine_first)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  dat2$bun_first and dat2$creatinine_first
## t = 41.694, df = 2397, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.6245445 0.6709689
## sample estimates:
##      cor 
## 0.648359</code></pre>
<pre class="r"><code>cor.test(dat2$bun_first,dat2$creatinine_first,method=&quot;spearman&quot;)</code></pre>
<pre><code>## Warning in cor.test.default(dat2$bun_first, dat2$creatinine_first, method =
## &quot;spearman&quot;): Cannot compute exact p-value with ties</code></pre>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  dat2$bun_first and dat2$creatinine_first
## S = 977030000, p-value &lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.5754121</code></pre>
<pre class="r"><code>cor.test(dat2$bun_first,dat2$creatinine_first,method=&quot;kendall&quot;)</code></pre>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  dat2$bun_first and dat2$creatinine_first
## z = 30.276, p-value &lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.4372488</code></pre>
<p>We can produce a scatterplot of the same two variables:</p>
<pre class="r"><code>plot(dat2$bun_first,dat2$creatinine_first)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-17-1.png" width="960" /></p>
<p>We can see that there is indeed a positive correlation between the two variables, but the data has more variability for higher values of <code>bun_first</code> and <code>creatinine_first</code>. It’s advisable to consider transformations of these two variables and be wary about using Pearson’s correlation.</p>
</div>
<div id="creating-categorical-variables-from-continuousnumeric-variables" class="section level2">
<h2>Creating Categorical Variables from Continuous/Numeric Variables</h2>
<p>Sometimes numeric variables need to be broken down into categorical variables or factors. This can be done for a variety of reasons. There is a useful function called <code>cut2</code> in the <code>Hmisc</code> package. We install it and use it below.</p>
<pre class="r"><code>if(!(&quot;Hmisc&quot; %in% installed.packages()[,1])) {
  install.packages(&quot;Hmisc&quot;)
}
library(Hmisc)

dat2$age.cat &lt;- cut2(dat2$age,g=5)
table(dat2$age.cat)</code></pre>
<pre><code>## 
## [16.0, 38.8) [38.8, 51.9) [51.9, 63.9) [63.9, 77.8) [77.8,300.0] 
##          551          550          550          550          550</code></pre>
<pre class="r"><code>dat2$age.cat2 &lt;- cut2(dat2$age,c(25,40,55,70,85))
table(dat2$age.cat2)</code></pre>
<pre><code>## 
## [ 16, 25) [ 25, 40) [ 40, 55) [ 55, 70) [ 70, 85) [ 85,300] 
##       238       352       652       635       630       244</code></pre>
<p><code>cut2</code> typically needs two arguments. The first is a numeric variable to convert into a factor, and the second is how to do the splitting. Specifying <code>g=5</code> (as above for <code>age.cat</code>) breaks the numeric variable into 5 groups, with the cut points determined by attempting to make the groups as equally sized as possible. As you can see in this example, due to the odd number of patients, they are not perfectly even. The second approach requires passing the cut points. In the second example, we tell <code>R</code> to cut the data at 25, 40,…. This results in 6 groups for five cut points.</p>
<div id="student-question-3" class="section level3">
<h3>Student Question 3:</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Create a new variable in the <code>dat2</code> data frame called <code>sofa.cat</code> made up of four (approx.) equally sized groups for SOFA. Print the sample size in each group. Why might the number in each group differ so much?</li>
<li>For each SOFA group calculate the number of people who survived and died in the hospital and at 28 days (use the <code>hosp_exp_flg</code> and <code>day_28_flg</code> variable).</li>
<li>Does mortality go up or down as SOFA increases?</li>
<li>Can you suggest a reason why we might want to convert <code>age</code> to a categorical variable instead of using it in it’s current form?</li>
</ol>
</blockquote>
<pre class="r"><code>dat2$sofa.cat &lt;- cut2(dat2$sofa_first,c(2,4,6,8,10)) # This is not the answer for part a).  Please amend to follow the instructions.</code></pre>
</div>
<div id="student-answer-3" class="section level3">
<h3>Student Answer 3:</h3>
<pre class="r"><code>dat2$sofa.cat &lt;- cut2(dat2$sofa_first, g=4)
table(dat2$sofa.cat)</code></pre>
<pre><code>## 
##      0 [1, 3) [3, 5) [5,14] 
##    853    599    674    625</code></pre>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>For the data points with the same value, we can’t separate them into two groups. For example, 853 patients has SOFA score = 0, but we can’t separate them and put some patients in the other group.</li>
</ol></li>
</ul>
<pre class="r"><code>CreateTableOne(vars=c(&quot;hosp_exp_flg&quot;, &quot;day_28_flg&quot;),
               strata=&quot;sofa.cat&quot;,
               data=dat2, test=FALSE)</code></pre>
<pre><code>##                       Stratified by sofa.cat
##                        0           [1, 3)      [3, 5)      [5,14]     
##   n                    853         599         674         625        
##   hosp_exp_flg = 1 (%) 108 (12.7)   67 (11.2)   78 (11.6)   97 (15.5) 
##   day_28_flg = 1 (%)   121 (14.2)   83 (13.9)   93 (13.8)  120 (19.2)</code></pre>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>We can use <code>CreateTableOne</code> to illustrate the answer.</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>The proportion of mortality goes up as SOFA increases.</li>
</ol></li>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>Grouping by <code>age</code> can reduce the unnecessary complex modeling. For example, patients with age of 63 or 64 may have similar underlying physical condition based on age, therefore it is reasonable to group them together.</li>
</ol></li>
</ul>
</div>
</div>
</div>
<div id="plotting-relationships-with-discrete-variables" class="section level1">
<h1>Plotting relationships with discrete variables</h1>
<p>Plotting discrete data can be a little tricky, but if done right can be very effective. For an example of why it’s difficult, using the basic plot command from last week, let’s plot two discrete variables: <code>gender_num</code> and <code>aline_flg</code>.</p>
<pre class="r"><code>plot(dat2$gender_num,dat2$aline_flg,xlab=&quot;Gender&quot;,ylab=&quot;IAC&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-22-1.png" width="960" /></p>
<p>Because we have converted <code>gender_num</code> and <code>aline_flg</code> to a factor, <code>R</code> gives us what is called a “Factor Plot”. The area of the light grey region is proportional to the proportion of each gender who received an aline. In this case, there is not that big of a difference between the genders.</p>
<p>This factor plot is more useful than if we were to keep the original numerical class both these variables had. For example, if we use the original <code>dat</code> data frame both <code>gender_num</code> and <code>aline_flg</code> are numeric variables, and when we plot these, we don’t end up with something very useful:</p>
<pre class="r"><code>plot(dat$gender_num,dat$aline_flg,xlab=&quot;Gender&quot;,ylab=&quot;IAC&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-23-1.png" width="960" /></p>
<p>In this case we only have four different types of data points, and although we could try to jitter the values to get a slightly more useful plot, it’s unlikely that this would give us a good visual interpretation of the data.</p>
<p>Sometimes the covariate may take on more than two levels. Here, we plot the in-hospital mortality rate by the different SOFA values, and put a smooth curve through the points. This covers a more <em>advanced</em> topic, and we <em><em>don’t</em></em> expect you to understand the technical details of the code below.</p>
<pre class="r"><code>plot(names(table(dat2$sofa_first)),sapply(split(dat2,dat2$sofa_first),function(x) { mean(x$hosp_exp_flg==1,na.rm=T)}),xlab=&quot;SOFA&quot;,ylab=&quot;In-Hospital Mortality&quot;,cex=log(as.numeric(table(dat2$sofa_first)+1))/5)
lines(smooth.spline(dat2$sofa_first,dat2$hosp_exp_flg==1),type=&quot;l&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-24-1.png" width="960" /></p>
<p>SOFA is a validated disease severity scale for the ICU, and generally correlates strongly with mortality. Here, while the mortality rate generally increases as SOFA increases, the smooth fit isn’t necessarily non-decreasing as SOFA values increase. We have added points roughly proportional to the sample size of each SOFA level, and you’ll see towards the high levels of SOFA, very few patients are observed, with the second highest score (13) having a 100% <em>survival</em> rate – although only in two patients!</p>
<p>For binary outcomes, it is often useful to plot the proportion of patients with the outcome (e.g., mortality rate) by the different levels of a covariate of interest. Because sample size plays such an important role in the uncertainty associated with these estimate proportions, it seems appropriate to include an estimate of our uncertainty via a confidence interval.</p>
<p>In the <code>MIMICbook</code> package you installed above, there is a <code>plot_prop_by_level</code> which can plot the proportion of patients with an outcome by one or two factor variables. For instance, if we wished to plot the in hospital mortality rate by the SOFA categories (<code>sofa.cat</code>) we defined above, we can using:</p>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;sofa.cat&quot;,&quot;hosp_exp_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-25-1.png" width="960" /></p>
<p>Often it’s useful to consider more than one covariate at a time to assess confounding and effect modification. Here, if we wished to examine <code>sofa.cat</code> and <code>gender_num</code> at the same time, we add <code>factor.var2=&quot;gender_num&quot;</code> to our previous use of <code>plot_prop_by_level</code>.</p>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;sofa.cat&quot;,&quot;hosp_exp_flg&quot;,factor.var2=&quot;gender_num&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-26-1.png" width="960" /></p>
<p>Here we see that in hospital mortality is higher in women in 3/4 of the SOFA groups we considered, suggesting that it might be an important confounder for this outcome and variable.</p>
<div id="student-question-4" class="section level3">
<h3>Student Question 4:</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Make a factor plot of the categories of SOFA we created (<code>sofa.cat</code>) and hospital mortality (<code>hosp_exp_flg</code>). Discuss what trend you observed.</li>
<li>Use <code>plot_prop_by_level</code> using <code>sofa.cat</code> as the covariate of interest and 28 day mortality (<code>day_28_flg</code>) as the outcome. How does this plot compare with part a)?</li>
<li>Include the main covariate of interest for this study <code>aline_flg</code> as the second factor variable and extend part b).</li>
<li>Repeat part c), but swap the aline and sofa arguments. Does this plot better illustrate our objective? Why or Why not?</li>
<li>Create a new variable, <code>sofa.cat2</code>, with cut points at 3, 6, 9, 12. Repeat parts b) and c). Is there a problem with these plots? Why is this case? Suggest a possible remedy.</li>
<li>Make a plot of the 28 day mortality outcome, <code>aline_flg</code> and <code>chf_flg</code>. Ignoring the statistical significance (i.e., do not perform any formal tests), comment on why this plot may suggest the complexity of any potential effect of aline.</li>
</ol>
</blockquote>
</div>
<div id="student-answer-4" class="section level3">
<h3>Student Answer 4:</h3>
<pre class="r"><code>plot(dat2$sofa.cat, dat2$hosp_exp_flg, xlab=&quot;SOFA category&quot;,ylab=&quot;Hospital mortality&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-27-1.png" width="960" /></p>
<ul>
<li><ol style="list-style-type: lower-alpha">
<li>The hospital mortality increases when the SOFA score increases (the group with higher SOFA score has less area of dark region).</li>
</ol></li>
</ul>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;sofa.cat&quot;, &quot;day_28_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-28-1.png" width="960" /></p>
<ul>
<li><ol start="2" style="list-style-type: lower-alpha">
<li>In this plot we only focus on the proportion of mortality (light region) so it become much clearer about the information of mortality proportion.</li>
</ol></li>
</ul>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;sofa.cat&quot;, &quot;day_28_flg&quot;, factor.var2=&quot;aline_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-29-1.png" width="960" /></p>
<ul>
<li><ol start="3" style="list-style-type: lower-alpha">
<li>In the group of SOFA score = 0, the proportion of using A-line is larger than not using A-line. But in other groups the proportion of using A-line is equal or smaller than not using A-line.</li>
</ol></li>
</ul>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;aline_flg&quot;, &quot;day_28_flg&quot;, factor.var2=&quot;sofa.cat&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-30-1.png" width="960" /></p>
<ul>
<li><ol start="4" style="list-style-type: lower-alpha">
<li>The plot is better because that the different SOFA categories were placed closer, and it can answer which SOFA category has higher mortality based on different A-line conditions.</li>
</ol></li>
</ul>
<pre class="r"><code>dat2$sofa.cat2 &lt;- cut2(dat2$sofa_first,c(3, 6, 9, 12))
plot_prop_by_level(dat2,&quot;sofa.cat2&quot;, &quot;day_28_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-31-1.png" width="960" /></p>
<ul>
<li><ol start="5" style="list-style-type: lower-alpha">
<li>The big problem of this approach is that the number of patients is unbalanced in different SOFA categories, and we can see that in the last group <code>[12, 14)</code> there are few patients and the error bar is long. We can’t make sure whether the category with the largest SOFA score has the great mortality. One solution is to separate the categorical variable (<code>sofa.cat</code>) with approximately equal number of the patients as we have done in (b) and (c), or maybe we can use different weights for different SOFA categories based on their number of patients. For example, for the categories with the number of patients of 1, 2 and 3, give them the weight adjustment of 6, 3, and 2, respectively.</li>
</ol></li>
</ul>
<pre class="r"><code>plot_prop_by_level(dat2,&quot;chf_flg&quot;, &quot;day_28_flg&quot;, factor.var2=&quot;aline_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-32-1.png" width="960" /></p>
<ul>
<li><ol start="6" style="list-style-type: lower-alpha">
<li>In this plot we observed that in the CHF patients, the subgroup of patients using A-line has less mortality, but in the non-CHF patients we didn’t see this phenomenon. The effect might because that the CHF patients with A-line insertion can monitor and report their blood pressure in time. The doctor can manage the patients immediately once the blood pressure becomes unstable. Therefore A-line insertion may be related to the reduction of mortality in CHF patients. However, unless the RCT is performed, we can not 100% sure that our causal inference is correct.</li>
</ol></li>
</ul>
</div>
<div id="odds-ratios" class="section level2">
<h2>Odds ratios</h2>
<p><em>Note: For those with a programming background, <code>R</code> indexes vectors starting from 1.</em></p>
<p>As discussed earlier today, odds ratios are very commonly used to communicate relative effect sizes for binary outcomes, particularly in observational data. Calculation is straightforward, but often misunderstood. We start with a 2 x 2 table. Below is the 2 x 2 table for in hospital mortality and having an arterial line. I’ve assigned it to a new variable called <code>egtab</code>.</p>
<pre class="r"><code>egtab &lt;- table(dat2$aline_flg,dat2$hosp_exp_flg,dnn=c(&quot;Aline&quot;,&quot;Hosp. Mort&quot;))
egtab</code></pre>
<pre><code>##      Hosp. Mort
## Aline    0    1
##     0 1193  165
##     1 1208  185</code></pre>
<p>It’s hard to interpret the raw counts, so we’ll use <code>prop.table</code> to compute the proportions who died and lived by row (margin 1, aline).</p>
<pre class="r"><code>pegtab &lt;- prop.table(egtab,1)
pegtab</code></pre>
<pre><code>##      Hosp. Mort
## Aline         0         1
##     0 0.8784978 0.1215022
##     1 0.8671931 0.1328069</code></pre>
<p>Odds are <span class="math inline">\(\frac{p}{1-p}\)</span> where <span class="math inline">\(p\)</span> is the proportion with the outcome (death) in a group of patients, which is in the second column. We can index the above table by column (<code>tab[,idx]</code> will retrieve column <code>idx</code> from the table [or matrix] <code>tab</code>) to compute the odds in each group.</p>
<pre class="r"><code>Oddsegtab &lt;-pegtab[,2]/pegtab[,1]
Oddsegtab</code></pre>
<pre><code>##         0         1 
## 0.1383068 0.1531457</code></pre>
<p>Now we have the odds of the outcome in those who got an aline <code>1</code> and those who didn’t <code>0</code>. We need to pick a reference group. We’ll calculate it both ways, but let’s assume we want those without an aline to be the reference:</p>
<pre class="r"><code>Oddsegtab[2]/Oddsegtab[1]</code></pre>
<pre><code>##       1 
## 1.10729</code></pre>
<p>If we wanted those with an aline to be the reference group:</p>
<pre class="r"><code>Oddsegtab[1]/Oddsegtab[2]</code></pre>
<pre><code>##        0 
## 0.903106</code></pre>
<p>If we wanted to plot this information, and include a confidence interval, we can use the <code>plot_OR_by_level</code> from the <code>MIMICbook</code> package:</p>
<pre class="r"><code>plot_OR_by_level(dat2,&quot;aline_flg&quot;,&quot;hosp_exp_flg&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-38-1.png" width="960" /></p>
<p>This by default includes an odds ratio of 1 indicating the reference group. To remove this point use the <code>include.ref.group.effect</code> argument:</p>
<pre class="r"><code>plot_OR_by_level(dat2,&quot;aline_flg&quot;,&quot;hosp_exp_flg&quot;,include.ref.group.effect = FALSE)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-39-1.png" width="960" /></p>
<p>You can also look at more than one covariate at a time. For instance, looking at <code>aline_flg</code> and the <code>gender_num</code> variable:</p>
<pre class="r"><code>plot_OR_by_level(dat2,&quot;gender_num&quot;,&quot;hosp_exp_flg&quot;,factor.var2=&quot;aline_flg&quot;,include.ref.group.effect = TRUE)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-40-1.png" width="960" /></p>
<p>Here we have computed the odds ratio for aline (vs no aline) separately for men and women.</p>
<div id="student-question-5" class="section level3">
<h3>Student Question 5:</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Create a 2 x 2 table with <code>chf_flg</code> and the variable <code>day_28_flg</code> outcome and assign it to a variable called <code>tab22</code></li>
<li>Compute the odds ratio for having CHF vs. not having CHF using this table.</li>
<li>Construct a plot of the odds ratios and 95% confidence intervals using the <code>plot_OR_by_level</code> function for CHF and 28 day mortality.</li>
<li>Create a 4 x 2 table with the <code>sofa.cat</code> variable and the <code>day_28_flg</code> outcome and assign it to a variable called <code>tab42</code>.</li>
<li>Pick and define a reference group for <code>sofa.cat</code>, and compute the odds ratio(s) for the other levels of <code>sofa.cat</code> using <code>day_28_flg</code> as your outcome.</li>
<li>Construct a plot of the odds ratios and 95% confidence intervals using the <code>plot_OR_by_level</code> function for the SOFA categories and 28 day mortality. Make sure the reference groups are the same as parts d) and e). Look into the <code>relevel</code> function in <code>R</code> or the <code>ref.group</code> argument in the <code>plot_OR_by_level</code> function.</li>
<li>Construct a plot looking at the 28 day mortality outcome, and the two variables we considered here, <code>sofa.cat</code> and <code>chf_flg</code>. Exchange the variables assigned to the factor.var1 and factor.var2 arguments, and discuss briefly two reasons why you prefer one plot over the other, and what you would conclude from your chosen plot.</li>
</ol>
</blockquote>
</div>
<div id="students-answers-5" class="section level3">
<h3>Students Answers 5:</h3>
<pre class="r"><code>tab22 &lt;- table(dat2$chf_flg, dat2$day_28_flg , dnn=c(&quot;chf&quot;,&quot;28d mort&quot;)) # a
(odds22 &lt;- tab22[, 2] / tab22[, 1]) # b</code></pre>
<pre><code>##         0         1 
## 0.1842971 0.1207729</code></pre>
<pre class="r"><code>plot_OR_by_level(dat2, &quot;chf_flg&quot;, &quot;day_28_flg&quot;, alpha=0.05) # c</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-41-1.png" width="960" /></p>
<pre class="r"><code>tab42 &lt;- table(dat2$sofa.cat, dat2$day_28_flg , dnn=c(&quot;sofa&quot;,&quot;28d mort&quot;)) # d
odds42 &lt;- tab42[, 2] / tab42[, 1]
odds42[2:4] / odds42[1] # e: use SOFA=0 as reference</code></pre>
<pre><code>##    [1, 3)    [3, 5)    [5,14] 
## 0.9730924 0.9683504 1.4375256</code></pre>
<pre class="r"><code>plot_OR_by_level(dat2, &quot;sofa.cat&quot;, &quot;day_28_flg&quot;, alpha=0.05) # f</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-41-2.png" width="960" /></p>
<pre class="r"><code>plot_prop_by_level(dat2, &quot;sofa.cat&quot;, &quot;day_28_flg&quot;, factor.var2=&quot;chf_flg&quot;) # g</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-41-3.png" width="960" /></p>
<pre class="r"><code>plot_prop_by_level(dat2, &quot;chf_flg&quot;, &quot;day_28_flg&quot;, factor.var2=&quot;sofa.cat&quot;)</code></pre>
<p><img src="workshop_9_eda_student_files/figure-html/unnamed-chunk-41-4.png" width="960" /></p>
<ul>
<li><ol start="7" style="list-style-type: lower-alpha">
<li>The choosing depends on the question we want to ask. If we want to know (1) the general trend of proportion of mortality between the patients with different SOFA score or (2) the difference of proportion between CHF or non-CHF status in different SOFA groups, then we can use the first plot. But if we want to see (1) the comparison between CHF and non-CHF regardless SOFA, or (2) the difference between different SOFA groups clearly, then the second plot is preferred. For example, from the second plot we can conclude that the non-CHF group has higher 28-day mortality in general, and the proportion of mortality increases when the SOFA score become higher.</li>
</ol></li>
</ul>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
